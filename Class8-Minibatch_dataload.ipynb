{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIUSSftwWwFEEayLhOhVXK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5_4CTjn9IpO","executionInfo":{"status":"ok","timestamp":1701843955826,"user_tz":300,"elapsed":1381,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}},"outputId":"caab335c-9c77-4a9b-fe97-7ac7876f539e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# 1. Mini batch and batch size"],"metadata":{"id":"eR2sFcCgBKXt"}},{"cell_type":"code","source":["import torch\n","\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"],"metadata":{"id":"X3i8UI_pBNtX","executionInfo":{"status":"ok","timestamp":1701843955827,"user_tz":300,"elapsed":12,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["While the example data set only contains five samples, there is a lot more data (e.g., millions) to deal with in real-world projects. Therefore, in many cases, training with the entire data set would be computationally too intensive or implausible.  \n","**Mini-batch** is a subset of the training data used in each iteration of the optimization algorithm.\n","\n","![picture](https://wikidocs.net/images/page/55580/%EB%AF%B8%EB%8B%88%EB%B0%B0%EC%B9%98.PNG)"],"metadata":{"id":"QKRjd7xHBaFc"}},{"cell_type":"markdown","source":["**Epoch**: one complete pass through the entire training dataset  \n","**Batch size**: size of the mini-batch  \n","**Minibatch Gradient Descent**: Gradient descent method performed by mini-batch unit"],"metadata":{"id":"Rjbl-D5OCFer"}},{"cell_type":"markdown","source":["# 2. Iteration"],"metadata":{"id":"qXlZvqxYELn9"}},{"cell_type":"markdown","source":["**Iteration** is the number of updating parameters (that is, weight and bias) in one epoch.\n","\n","![picture](https://wikidocs.net/images/page/36033/batchandepochiteration.PNG)\n"],"metadata":{"id":"q3kBBtXsEQC1"}},{"cell_type":"markdown","source":["# 3. Data load"],"metadata":{"id":"h9E7zFujFJMG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# import TensorDataset and DataLoader\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"_xX81UCJCKoi","executionInfo":{"status":"ok","timestamp":1701843955827,"user_tz":300,"elapsed":11,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# train data\n","x_train  =  torch.FloatTensor([[73,  80,  75],\n","                               [93,  88,  93],\n","                               [89,  91,  90],\n","                               [96,  98,  100],\n","                               [73,  66,  70]])\n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"],"metadata":{"id":"4R_1c_mDCEnv","executionInfo":{"status":"ok","timestamp":1701843955827,"user_tz":300,"elapsed":9,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["**TensorDatset** takes tensors as input and transforms them into a data set."],"metadata":{"id":"xhniZGzhh5Vw"}},{"cell_type":"code","source":["# save the train data to \"dataset\" object\n","dataset = TensorDataset(x_train, y_train)"],"metadata":{"id":"H9NKjG-yB5Dv","executionInfo":{"status":"ok","timestamp":1701843955827,"user_tz":300,"elapsed":8,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["**DataLoader** has two parameters: data set and mini-batch size. Mini-bach size is normally multiple of two."],"metadata":{"id":"q_oHwIZjiThy"}},{"cell_type":"code","source":["# save the data set and mini-batch information to \"dataloader\" object\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"],"metadata":{"id":"3iYSFI7BijzA","executionInfo":{"status":"ok","timestamp":1701843955827,"user_tz":300,"elapsed":7,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# initiate the model and optimizer\n","model = nn.Linear(3,1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"],"metadata":{"id":"0HqUEr13jcwi","executionInfo":{"status":"ok","timestamp":1701843955827,"user_tz":300,"elapsed":6,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["for batch_idx, samples in enumerate(dataloader):\n","  print(samples[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4kvPoeBk7NE","executionInfo":{"status":"ok","timestamp":1701843955828,"user_tz":300,"elapsed":7,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}},"outputId":"a7f407f3-33f0-4ab9-e278-1d1dcf779acb"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[89., 91., 90.],\n","        [73., 80., 75.]])\n","tensor([[ 93.,  88.,  93.],\n","        [ 96.,  98., 100.]])\n","tensor([[73., 66., 70.]])\n"]}]},{"cell_type":"code","source":["# train the model\n","nb_epochs = 20\n","\n","for epoch in range(nb_epochs + 1):\n","  # check how mini-batch concepts work\n","  for batch_idx, samples in enumerate(dataloader):\n","    print(batch_idx)\n","    print(samples)\n","\n","    x_train, y_train = samples    # divide samples into x_train and y_train\n","\n","    # H(x)\n","    prediction = model(x_train)   # same as model.forward(x_train)\n","\n","    # cost calculation\n","    cost = F.mse_loss(prediction, y_train)\n","\n","    # update H(x) using cost\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, batch_idx+1, len(dataloader),\n","        cost.item()\n","        ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hp2k6AKJjtD0","executionInfo":{"status":"ok","timestamp":1701843956060,"user_tz":300,"elapsed":237,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}},"outputId":"3c8eeae7-0dfe-44a8-8404-2c879a8017e2"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","[tensor([[ 73.,  80.,  75.],\n","        [ 96.,  98., 100.]]), tensor([[152.],\n","        [196.]])]\n","Epoch    0/20 Batch 1/3 Cost: 17060.908203\n","1\n","[tensor([[89., 91., 90.],\n","        [93., 88., 93.]]), tensor([[180.],\n","        [185.]])]\n","Epoch    0/20 Batch 2/3 Cost: 5171.281250\n","2\n","[tensor([[73., 66., 70.]]), tensor([[142.]])]\n","Epoch    0/20 Batch 3/3 Cost: 771.635071\n","0\n","[tensor([[73., 66., 70.],\n","        [89., 91., 90.]]), tensor([[142.],\n","        [180.]])]\n","Epoch    1/20 Batch 1/3 Cost: 526.245911\n","1\n","[tensor([[ 96.,  98., 100.],\n","        [ 73.,  80.,  75.]]), tensor([[196.],\n","        [152.]])]\n","Epoch    1/20 Batch 2/3 Cost: 261.398132\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch    1/20 Batch 3/3 Cost: 66.357750\n","0\n","[tensor([[ 93.,  88.,  93.],\n","        [ 96.,  98., 100.]]), tensor([[185.],\n","        [196.]])]\n","Epoch    2/20 Batch 1/3 Cost: 17.800320\n","1\n","[tensor([[73., 66., 70.],\n","        [89., 91., 90.]]), tensor([[142.],\n","        [180.]])]\n","Epoch    2/20 Batch 2/3 Cost: 2.820609\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch    2/20 Batch 3/3 Cost: 8.661658\n","0\n","[tensor([[73., 80., 75.],\n","        [73., 66., 70.]]), tensor([[152.],\n","        [142.]])]\n","Epoch    3/20 Batch 1/3 Cost: 1.873445\n","1\n","[tensor([[89., 91., 90.],\n","        [93., 88., 93.]]), tensor([[180.],\n","        [185.]])]\n","Epoch    3/20 Batch 2/3 Cost: 0.135014\n","2\n","[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n","Epoch    3/20 Batch 3/3 Cost: 0.078776\n","0\n","[tensor([[73., 66., 70.],\n","        [93., 88., 93.]]), tensor([[142.],\n","        [185.]])]\n","Epoch    4/20 Batch 1/3 Cost: 0.038502\n","1\n","[tensor([[ 73.,  80.,  75.],\n","        [ 96.,  98., 100.]]), tensor([[152.],\n","        [196.]])]\n","Epoch    4/20 Batch 2/3 Cost: 1.926003\n","2\n","[tensor([[89., 91., 90.]]), tensor([[180.]])]\n","Epoch    4/20 Batch 3/3 Cost: 0.071313\n","0\n","[tensor([[73., 80., 75.],\n","        [73., 66., 70.]]), tensor([[152.],\n","        [142.]])]\n","Epoch    5/20 Batch 1/3 Cost: 1.592341\n","1\n","[tensor([[ 96.,  98., 100.],\n","        [ 89.,  91.,  90.]]), tensor([[196.],\n","        [180.]])]\n","Epoch    5/20 Batch 2/3 Cost: 0.286765\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch    5/20 Batch 3/3 Cost: 0.099987\n","0\n","[tensor([[ 73.,  66.,  70.],\n","        [ 96.,  98., 100.]]), tensor([[142.],\n","        [196.]])]\n","Epoch    6/20 Batch 1/3 Cost: 0.055332\n","1\n","[tensor([[89., 91., 90.],\n","        [73., 80., 75.]]), tensor([[180.],\n","        [152.]])]\n","Epoch    6/20 Batch 2/3 Cost: 1.892039\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch    6/20 Batch 3/3 Cost: 0.237704\n","0\n","[tensor([[ 93.,  88.,  93.],\n","        [ 96.,  98., 100.]]), tensor([[185.],\n","        [196.]])]\n","Epoch    7/20 Batch 1/3 Cost: 0.063550\n","1\n","[tensor([[89., 91., 90.],\n","        [73., 66., 70.]]), tensor([[180.],\n","        [142.]])]\n","Epoch    7/20 Batch 2/3 Cost: 0.033023\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch    7/20 Batch 3/3 Cost: 3.708516\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 93.,  88.,  93.]]), tensor([[196.],\n","        [185.]])]\n","Epoch    8/20 Batch 1/3 Cost: 0.829037\n","1\n","[tensor([[73., 80., 75.],\n","        [89., 91., 90.]]), tensor([[152.],\n","        [180.]])]\n","Epoch    8/20 Batch 2/3 Cost: 1.390938\n","2\n","[tensor([[73., 66., 70.]]), tensor([[142.]])]\n","Epoch    8/20 Batch 3/3 Cost: 0.478148\n","0\n","[tensor([[93., 88., 93.],\n","        [73., 66., 70.]]), tensor([[185.],\n","        [142.]])]\n","Epoch    9/20 Batch 1/3 Cost: 0.210130\n","1\n","[tensor([[ 89.,  91.,  90.],\n","        [ 96.,  98., 100.]]), tensor([[180.],\n","        [196.]])]\n","Epoch    9/20 Batch 2/3 Cost: 0.031495\n","2\n","[tensor([[73., 80., 75.]]), tensor([[152.]])]\n","Epoch    9/20 Batch 3/3 Cost: 3.482345\n","0\n","[tensor([[73., 80., 75.],\n","        [93., 88., 93.]]), tensor([[152.],\n","        [185.]])]\n","Epoch   10/20 Batch 1/3 Cost: 1.165751\n","1\n","[tensor([[ 73.,  66.,  70.],\n","        [ 96.,  98., 100.]]), tensor([[142.],\n","        [196.]])]\n","Epoch   10/20 Batch 2/3 Cost: 0.906641\n","2\n","[tensor([[89., 91., 90.]]), tensor([[180.]])]\n","Epoch   10/20 Batch 3/3 Cost: 0.114531\n","0\n","[tensor([[93., 88., 93.],\n","        [89., 91., 90.]]), tensor([[185.],\n","        [180.]])]\n","Epoch   11/20 Batch 1/3 Cost: 0.071180\n","1\n","[tensor([[73., 80., 75.],\n","        [73., 66., 70.]]), tensor([[152.],\n","        [142.]])]\n","Epoch   11/20 Batch 2/3 Cost: 1.690358\n","2\n","[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n","Epoch   11/20 Batch 3/3 Cost: 0.318659\n","0\n","[tensor([[ 93.,  88.,  93.],\n","        [ 96.,  98., 100.]]), tensor([[185.],\n","        [196.]])]\n","Epoch   12/20 Batch 1/3 Cost: 0.052696\n","1\n","[tensor([[73., 80., 75.],\n","        [73., 66., 70.]]), tensor([[152.],\n","        [142.]])]\n","Epoch   12/20 Batch 2/3 Cost: 1.831560\n","2\n","[tensor([[89., 91., 90.]]), tensor([[180.]])]\n","Epoch   12/20 Batch 3/3 Cost: 0.082870\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 93.,  88.,  93.]]), tensor([[196.],\n","        [185.]])]\n","Epoch   13/20 Batch 1/3 Cost: 0.102902\n","1\n","[tensor([[89., 91., 90.],\n","        [73., 80., 75.]]), tensor([[180.],\n","        [152.]])]\n","Epoch   13/20 Batch 2/3 Cost: 1.737631\n","2\n","[tensor([[73., 66., 70.]]), tensor([[142.]])]\n","Epoch   13/20 Batch 3/3 Cost: 0.325848\n","0\n","[tensor([[89., 91., 90.],\n","        [73., 80., 75.]]), tensor([[180.],\n","        [152.]])]\n","Epoch   14/20 Batch 1/3 Cost: 1.488555\n","1\n","[tensor([[ 96.,  98., 100.],\n","        [ 93.,  88.,  93.]]), tensor([[196.],\n","        [185.]])]\n","Epoch   14/20 Batch 2/3 Cost: 0.435211\n","2\n","[tensor([[73., 66., 70.]]), tensor([[142.]])]\n","Epoch   14/20 Batch 3/3 Cost: 0.149198\n","0\n","[tensor([[73., 80., 75.],\n","        [73., 66., 70.]]), tensor([[152.],\n","        [142.]])]\n","Epoch   15/20 Batch 1/3 Cost: 1.766319\n","1\n","[tensor([[89., 91., 90.],\n","        [93., 88., 93.]]), tensor([[180.],\n","        [185.]])]\n","Epoch   15/20 Batch 2/3 Cost: 0.164812\n","2\n","[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n","Epoch   15/20 Batch 3/3 Cost: 0.093804\n","0\n","[tensor([[89., 91., 90.],\n","        [73., 66., 70.]]), tensor([[180.],\n","        [142.]])]\n","Epoch   16/20 Batch 1/3 Cost: 0.032517\n","1\n","[tensor([[73., 80., 75.],\n","        [93., 88., 93.]]), tensor([[152.],\n","        [185.]])]\n","Epoch   16/20 Batch 2/3 Cost: 1.834235\n","2\n","[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n","Epoch   16/20 Batch 3/3 Cost: 0.245579\n","0\n","[tensor([[ 73.,  80.,  75.],\n","        [ 96.,  98., 100.]]), tensor([[152.],\n","        [196.]])]\n","Epoch   17/20 Batch 1/3 Cost: 1.676635\n","1\n","[tensor([[89., 91., 90.],\n","        [73., 66., 70.]]), tensor([[180.],\n","        [142.]])]\n","Epoch   17/20 Batch 2/3 Cost: 0.215644\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch   17/20 Batch 3/3 Cost: 0.101644\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 73.,  66.,  70.]]), tensor([[196.],\n","        [142.]])]\n","Epoch   18/20 Batch 1/3 Cost: 0.056469\n","1\n","[tensor([[73., 80., 75.],\n","        [89., 91., 90.]]), tensor([[152.],\n","        [180.]])]\n","Epoch   18/20 Batch 2/3 Cost: 1.863550\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch   18/20 Batch 3/3 Cost: 0.231834\n","0\n","[tensor([[ 96.,  98., 100.],\n","        [ 73.,  80.,  75.]]), tensor([[196.],\n","        [152.]])]\n","Epoch   19/20 Batch 1/3 Cost: 1.609585\n","1\n","[tensor([[73., 66., 70.],\n","        [89., 91., 90.]]), tensor([[142.],\n","        [180.]])]\n","Epoch   19/20 Batch 2/3 Cost: 0.236887\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch   19/20 Batch 3/3 Cost: 0.112046\n","0\n","[tensor([[89., 91., 90.],\n","        [73., 66., 70.]]), tensor([[180.],\n","        [142.]])]\n","Epoch   20/20 Batch 1/3 Cost: 0.042345\n","1\n","[tensor([[ 96.,  98., 100.],\n","        [ 73.,  80.,  75.]]), tensor([[196.],\n","        [152.]])]\n","Epoch   20/20 Batch 2/3 Cost: 1.781268\n","2\n","[tensor([[93., 88., 93.]]), tensor([[185.]])]\n","Epoch   20/20 Batch 3/3 Cost: 0.217415\n"]}]},{"cell_type":"code","source":["# random input: [73, 80, 75]\n","new_var =  torch.FloatTensor([[73, 80, 75]])\n","# get prediction for the random input above\n","pred_y = model(new_var)\n","print(\"prediction for [73, 80, 75]:\", pred_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mtJHDCEKzR2V","executionInfo":{"status":"ok","timestamp":1701843956060,"user_tz":300,"elapsed":7,"user":{"displayName":"Jinwoong Nam","userId":"11792221848563275675"}},"outputId":"00586cea-0af2-4a36-c9fd-f4ae096f78f2"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["prediction for [73, 80, 75]: tensor([[150.2217]], grad_fn=<AddmmBackward0>)\n"]}]}]}